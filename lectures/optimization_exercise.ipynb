{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Exercise: _Optimization_\n",
    "***\n",
    "\n",
    "We will implement various optimization algorithms and examine their performance for various tasks.\n",
    "\n",
    "\n",
    "1. First-order, smooth optimization using gradient descent \n",
    "    - Implement basic gradient descent solver\n",
    "    - Implement gradient descent with armijo backtracking\n",
    "2. Smooth One-class SVM\n",
    "    - Implement hinge- and Huber loss functions\n",
    "    - Implement objective and derivative of the smooth one-class svm\n",
    "    - Use check_grad to verify the implementation\n",
    "3. First-order, non-smooth optimization using sub-gradient descent\n",
    "    - Implement objective and derivative of $\\ell_p$-norm regularized one-class svm\n",
    "4. Utilizing Available QP Solver Packages: CVXOPT\n",
    "    - Use cvxopt qp solver to solve the primal one-class svm optimization problem\n",
    "5. Utilizing Available Solver Packages: SciPy's Optimization Suite\n",
    "    - Apply scipy's _minimize_ function on your implementation of the objective function of the smooth one-class svm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from scipy.optimize import check_grad, minimize\n",
    "\n",
    "import numpy as np\n",
    "import cvxopt as cvx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First-order, smooth optimization using gradient descent \n",
    "\n",
    "In this first part, we want use various variants of gradient descent for continuous and \n",
    "smooth optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A well-known continuous, convex, smooth method is l2-norm regularized logistic regression.\n",
    "Which has the following objective function:\n",
    "\n",
    "$f(w) = \\frac{\\lambda}{2} \\|w\\|^2 + \\sum_{i=1}^n \\log(1+\\exp(-y_i\\langle w, x_i \\rangle))$\n",
    "\n",
    "In order to apply gradient descent, we will further need the first derivative:\n",
    "\n",
    "$f'(w) = \\lambda w + \\sum_{i=1}^n \\frac{-y_i}{1+\\exp(y_i(\\langle w, x_i \\rangle))}x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_l2_logistic_regression(w, X, y, param):\n",
    "    w = w.reshape(w.size, 1)\n",
    "    t1 = 1. + np.exp(-y * w.T.dot(X).T)\n",
    "    f = param/2.*w.T.dot(w) + np.sum(np.log(t1))\n",
    "    return f[0,0]\n",
    "\n",
    "def grad_l2_logistic_regression(w, X, y, param):\n",
    "    w = w.reshape(w.size, 1)\n",
    "    t2 = 1. + np.exp(y * w.T.dot(X).T)\n",
    "    grad = param*w + (-y/t2).T.dot(X.T).T\n",
    "    return grad.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a basic gradient descent solver. Plot iterations number, \n",
    "objective function, and stopping condition for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMin0(fun, grad, w, alpha, max_evals=10, eps=1e-2, verbosity=1):\n",
    "    # TODO \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let us generate some small data set to try out our optimization schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate some test data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 100)\n",
    "w = np.random.randn(10, 1)\n",
    "reg_y = w.T.dot(X)\n",
    "median = np.median(reg_y)\n",
    "y = -np.ones((reg_y.size, 1), dtype=np.int)\n",
    "y[reg_y.ravel() >= median] = +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a look at the most basic gradient descent method we can think of and \n",
    "start playing with the step-size $\\alpha$. Try some, e.g.\n",
    "- $\\alpha=1.0$ \n",
    "- $\\alpha=1e-6$\n",
    "- $\\alpha=0.001$\n",
    "\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fun = partial(fun_l2_logistic_regression, X=X, y=y, param=1.)\n",
    "grad = partial(grad_l2_logistic_regression, X=X, y=y, param=1.)\n",
    "\n",
    "print(check_grad(fun, grad, 0.0*np.random.randn(10)))\n",
    "\n",
    "wstar = findMin0(fun, grad, 0.0*np.random.randn(10), 0.001, max_evals=1000, eps=1e-8, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to tweak the $\\alpha$'s for every single optimization problem. This is where\n",
    "line search steps in. \n",
    "\n",
    "Implement a basic gradient descent solver with Armijo back-tracking line-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMinBT(fun, grad, w, alpha, gamma, max_evals=10, eps=1e-2, verbosity=1):\n",
    "    # TODO\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fun = partial(fun_l2_logistic_regression, X=X, y=y, param=1.)\n",
    "grad = partial(grad_l2_logistic_regression, X=X, y=y, param=1.)\n",
    "\n",
    "print(check_grad(fun, grad, 0.0*np.random.randn(10)))\n",
    "wstar = findMinBT(fun, grad, 0.0*np.random.randn(10), 1., 0.0001, max_evals=100, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More elaborate optimization methods (e.g. Newton descent) will use second-order information in order to find a better step-length. We will come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Smooth One-class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an anomaly detection workshop, we want to train some anomaly detectors.\n",
    "So, here is our one-class SVM primal problem again:\n",
    "\n",
    "$\\min_{w,\\rho,\\xi} \\frac{1}{2}\\|w\\|^2 - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n \\xi_i$\n",
    "\n",
    "subject to the following constraints:\n",
    "\n",
    "$\\xi_i \\geq 0\\;, \\quad \\langle w, x_i \\rangle \\geq \\rho - \\xi_i \\; , \\quad \\forall \\; i$\n",
    "\n",
    "This OP is unfortunately neither smooth nor unconstrained. So, lets change this.\n",
    "1. We will get rid of the constraints by re-formulating them \n",
    "$\\xi_i \\geq 0\\;, \\quad \\langle w, x_i \\rangle \\geq \\rho - \\xi_i \\; \\Rightarrow \n",
    "\\xi_i \\geq 0\\;, \\quad \\xi_i \\geq \\rho - \\langle w, x_i \\rangle$\n",
    "Since we minimize the objective, the RHS will hold with equality. Hence we can\n",
    "replace $\\xi_i$ in the objective with the RHS. However, we need to take care also of the\n",
    "LHS which states that if LHS is smaller than $0$ the value should stay $0$. This can be achieved\n",
    "by taking a $max(0, RHS)$. Hence, we land at the following _unconstrained_ problem:\n",
    "\n",
    "    $\\min_{w,\\rho} \\frac{1}{2}\\|w\\|^2 - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n max(0,\\rho - \\langle w, x_i \\rangle)$,\n",
    "\n",
    "    which can be written in terms of a general loss function $\\ell$ as\n",
    "\n",
    "    $\\min_{w,\\rho} \\frac{1}{2}\\|w\\|^2 - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n \\ell(\\rho - \\langle w, x_i \\rangle)$.\n",
    "\n",
    "\n",
    "    This is now unconstrained but still not smooth as the max (which is BTW called the hinge-loss) introduces some non-smoothness into the problem and gradient descent solvers can not readily applied. So, lets make it differentiable by approximation.\n",
    "    \n",
    "    \n",
    "2. Approximating the hinge-loss by differentiable Huber-loss\n",
    "\n",
    "    $\\ell_{\\Delta,\\epsilon}(x) := \n",
    "        \\left\\{\\begin{array}{lr}\n",
    "        \\Delta + x, & \\text{for } x \\geq \\Delta - \\epsilon\\\\\n",
    "        \\frac{(\\Delta + \\epsilon + x)^2}{4\\epsilon}, & \\text{for } \\Delta - \\epsilon\\leq x\\leq \\Delta + \\epsilon\\\\\n",
    "        0, & \\text{else}\n",
    "        \\end{array}\\right\\}$\n",
    "        \n",
    "    ..and the corresponding derivative is (I hope):\n",
    "   \n",
    "    $\\frac{\\partial}{\\partial x}\\ell_{\\Delta,\\epsilon}(x) := \n",
    "        \\left\\{\\begin{array}{lr}\n",
    "        1, & \\text{for } x \\geq \\Delta - \\epsilon\\\\\n",
    "        \\frac{(\\Delta + \\epsilon + x)}{2\\epsilon}, & \\text{for } \\Delta - \\epsilon\\leq x\\leq \\Delta + \\epsilon\\\\\n",
    "        0, & \\text{else}\n",
    "        \\end{array}\\right\\}$\n",
    "    \n",
    "    For our purposes, $\\Delta=0.0$ and $\\epsilon=0.5$ will suffice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Implement the hinge loss $\\ell(x) = \\max(0,x)$\n",
    "\n",
    "(2) Implement the Huber loss as defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(x):\n",
    "    # TODO\n",
    "    return 0\n",
    "    \n",
    "def huber_loss(x, delta, epsilon):\n",
    "    # TODO\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-1, +1, 1000)\n",
    "\n",
    "plt.plot(xs, hinge_loss(xs), '-r', linewidth=2.0)\n",
    "plt.plot(xs, huber_loss(xs, 0., 0.5), '--b', linewidth=2.0)\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the smooth one-class svm objective and derivative as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_smooth_ocsvm(var, X, nu, delta, epsilon):\n",
    "    # TODO\n",
    "    return 0\n",
    "    \n",
    "def grad_smooth_ocsvm(var, X, nu, delta, epsilon):\n",
    "    # TODO\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fun = partial(fun_smooth_ocsvm, X=X, nu=1.0, delta=0., epsilon=0.5)\n",
    "grad = partial(grad_smooth_ocsvm, X=X, nu=1.0, delta=0., epsilon=0.5)\n",
    "\n",
    "# First, check gradient vs numerical gradient.\n",
    "# This should give very small results.\n",
    "print(check_grad(fun, grad, 0.1*np.random.randn(10+1)))\n",
    "\n",
    "xstar = findMinBT(fun, grad, 0.0*np.random.randn(10+1), 1., 0.0001, max_evals=1000, eps=1e-4, verbosity=0)\n",
    "wstar = xstar[1:]\n",
    "\n",
    "print(wstar)\n",
    "print(np.mean(X, axis=1))\n",
    "print(np.linalg.norm(wstar - np.mean(X, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. First-order, non-smooth optimization using sub-gradient descent \n",
    "\n",
    "Unfortunately, many interesting methods do contain a non-smooth part in their objective. Examples \n",
    "include support vector machines (SVMs), one-class support vector machines (OCSVM), and support vector data descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we gonna implement a version of the primal one-class SVM with a $\\ell_p$-norm \n",
    "regularizer. This will allow us to control the sparsity of the found solution vector:\n",
    "\n",
    "$\\min_{w,\\rho} \\frac{1}{2}\\|w\\|_p^2 - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n max(0,\\rho - \\langle w, x_i \\rangle)$,\n",
    "\n",
    "The resulting optimization problem is unconstrained but non-smooth. We will use a subgradient descent solver \n",
    "for this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMinSG(fun, grad, x0, rate, max_evals=1000, eps=1e-2, step_method=1, verbosity=1):\n",
    "    dims = x0.size\n",
    "    x = x0\n",
    "    best_x = x\n",
    "    best_obj = np.float64(1e20)\n",
    "    obj_bak = -1e10\n",
    "    evals = 0\n",
    "    is_converged = False\n",
    "    while not is_converged and evals < max_evals:\n",
    "        obj = fun(x)\n",
    "        # this is subgradient, hence need to store the best solution so far\n",
    "        if best_obj >= obj:\n",
    "            best_x = x\n",
    "            best_obj = obj\n",
    "\n",
    "        # stop, if progress is too slow\n",
    "        if np.abs((obj-obj_bak)) < eps:\n",
    "            is_converged = True\n",
    "            continue\n",
    "        obj_bak = obj\n",
    "        \n",
    "        # gradient step for threshold\n",
    "        g = grad(x)\n",
    "        if step_method == 1:\n",
    "            # constant step\n",
    "            alpha = rate\n",
    "        elif step_method == 2:\n",
    "            # non-summable dimishing step size\n",
    "            alpha = rate / np.sqrt(np.float(evals+1.))\n",
    "        else:\n",
    "            # const. step length\n",
    "            alpha = rate / np.linalg.norm(g)\n",
    "            \n",
    "        if verbosity > 0:\n",
    "            print('{0} {1:5.5f} {2:5.5f} {3:5.5f}'.format(evals, alpha, obj, np.abs((obj-obj_bak))))           \n",
    "\n",
    "        # update\n",
    "        x = x - alpha*g\n",
    "        evals += 1\n",
    "\n",
    "    print('{0} {1:5.5f} {2:5.5f} {3:5.5f}'.format(evals, alpha, obj, np.abs((obj-obj_bak))))           \n",
    "    return best_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_lp_norm_ocsvm(var, X, p, nu):\n",
    "    # TODO\n",
    "    return 0\n",
    "    \n",
    "def grad_lp_norm_ocsvm(var, X, p, nu):\n",
    "    # TODO\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fun = partial(fun_lp_norm_ocsvm, X=X, p=2.0, nu=1.0)\n",
    "grad = partial(grad_lp_norm_ocsvm, X=X, p=2.0, nu=1.0)\n",
    "\n",
    "xstar = findMinSG(fun, grad, np.random.randn(10+1), 0.01, max_evals=2000, eps=1e-2, step_method=1, verbosity=1)\n",
    "wstar = xstar[1:]\n",
    "\n",
    "print(wstar)\n",
    "print(np.mean(X, axis=1))\n",
    "print(np.linalg.norm(wstar - np.mean(X, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on how the sparsity is controlled by varying $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "xs = np.array([1.0, 1.5, 2.0, 4.0, 100.0])\n",
    "sparsity = np.zeros((xs.size, X.shape[0]))\n",
    "for i in range(xs.size):\n",
    "    fun = partial(fun_lp_norm_ocsvm, X=X, p=xs[i], nu=1.0)\n",
    "    grad = partial(grad_lp_norm_ocsvm, X=X, p=xs[i], nu=1.0)\n",
    "\n",
    "    xstar = findMinSG(fun, grad, np.random.randn(10+1), 0.001, max_evals=5000, eps=1e-3, step_method=3, verbosity=0)\n",
    "    wstar = xstar[1:]\n",
    "\n",
    "    wstar = np.abs(wstar)\n",
    "    wstar /= np.max(wstar)\n",
    "    sparsity[i, :] = wstar\n",
    "\n",
    "    plt.subplot(1, xs.size, i+1)\n",
    "    plt.bar(np.arange(X.shape[0]), sparsity[i, :])\n",
    "    plt.title('p={0:1.2f}'.format(xs[i]))\n",
    "    plt.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Utilizing Available QP Solver Packages: CVXOPT\n",
    "\n",
    "There are very good general purpose solver for certain types of optimization problems \n",
    "available. Most important are cplex, mosek, and cvxopt where the latter is for free and \n",
    "contains interfaces for comercial solvers (cplex and mosek)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back again at the one-class SVM primal problem:\n",
    "\n",
    "$\\min_{w,\\rho,\\xi} \\frac{1}{2}\\|w\\|^2 - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n \\xi_i$\n",
    "\n",
    "subject to the following constraints:\n",
    "\n",
    "$\\xi_i \\geq 0\\;, \\quad \\langle w, x_i \\rangle \\geq \\rho - \\xi_i \\; , \\quad \\forall \\; i$\n",
    "\n",
    "Use cvxopt's qp method to solve this problem (cvxopt.solvers.qp(P, q, G, h))\n",
    "Hence, the above problem needs to be re-written as:\n",
    "\n",
    "$\\min_x \\frac{1}{2}x^T P x + q^T x$ subject to $Gx \\leq h$ and $Ax=b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_primal_qp_solution(X, nu):\n",
    "    # Solution vector 'x' is a concatenation of w \\in R^dims, xi \\inR^n, rho \\in R\n",
    "    # and hence has a dimensionality of dims+n+rho.\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    # solve qp\n",
    "    sol = cvx.solvers.qp(cvx.matrix(P), cvx.matrix(q), cvx.matrix(G), cvx.matrix(h))\n",
    "    return np.array(sol['x'])[:d].ravel(), np.array(sol['x'])[d:d+n].ravel(), np.array(sol['x'])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wstar, xistar, rhostar = calculate_primal_qp_solution(X, 1.)\n",
    "\n",
    "print('Optimized solution: ', wstar)\n",
    "print('Truth: ', np.mean(X, axis=1))\n",
    "print('Difference: ', np.linalg.norm(wstar - np.mean(X, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might notice, coding the derivatives is not trivial and takes the most of the time. There are some methods build-in scipy that help you with that and also optimize using more elaborate techniques such as second-order L-BFGS (a memory-limited newton descent). Here, let's recycle some of our functions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Utilizing Available Solver Packages: SciPy's Optimization Suite\n",
    "\n",
    "Here is a link to the scipy 'minimize' function which implements lots of solvers for smooth (un-)constrained\n",
    "optimization problems:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "\n",
    "We will recycle our smooth one-class SVM objective function.\n",
    "\n",
    "Use _L-BFGS-B_ as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 100)\n",
    "\n",
    "fun = partial(fun_smooth_ocsvm, X=X, nu=1.0, delta=0., epsilon=0.5)\n",
    "\n",
    "\n",
    "# res: result as returned by scipy\n",
    "xstar = res.x\n",
    "wstar = xstar[1:]\n",
    "\n",
    "print(wstar)\n",
    "print(np.mean(X, axis=1))\n",
    "print(np.linalg.norm(wstar - np.mean(X, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
