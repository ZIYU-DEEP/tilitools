{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Beginner's Workshop on Machine Learning: _Kernels_\n",
    "***\n",
    "\n",
    "This tutorial is basically split into four parts:\n",
    "1. Kernel Methods\n",
    "2. Standard Kernels\n",
    "3. Learning Kernels\n",
    "4. Approximating Kernels\n",
    "\n",
    "Throughout all parts, four distinct datasets can be used to \n",
    "examine various settings. Descriptions of each of the datasets\n",
    "can be found on the respective website:\n",
    "\n",
    "1. MNIST (http://yann.lecun.com/exdb/mnist/)\n",
    "2. EMNIST (https://www.nist.gov/itl/iad/image-group/emnist-dataset)\n",
    "3. Fashion-MNIST (https://github.com/zalandoresearch/fashion-mnist)\n",
    "4. Cifar10 (https://www.cs.toronto.edu/~kriz/cifar.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.svm as svm\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.kernel_approximation as approx\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Functions for loading the various datasets. E.g. MNIST: This well-known dataset consists of hand-written digits and hence, splits into 10 classes ('0'-'9'). Moreover, train- and testsets are separated and contain 60.000 (10.000) examples in total. For more information about the datasets, follow one of the links above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset():\n",
    "    import torchvision.datasets as datasets\n",
    "\n",
    "    mnist_train = datasets.MNIST(root='./data/mnist', train=True, download=True, transform=None)\n",
    "    mnist_test = datasets.MNIST(root='./data/mnist', train=False, download=True, transform=None)\n",
    "    test_labels = np.array([mnist_test[i][1].numpy() for i in range(len(mnist_test))], dtype=np.int)\n",
    "    train_labels = np.array([mnist_train[i][1].numpy() for i in range(len(mnist_train))], dtype=np.int)\n",
    "    test = np.array([np.asarray(mnist_test[i][0]).reshape(28*28) for i in range(len(mnist_test))], dtype=np.float)\n",
    "    train = np.array([np.asarray(mnist_train[i][0]).reshape(28*28) for i in range(len(mnist_train))], dtype=np.float)\n",
    "    train /= 255.  # normalize data to be in range [0,1]\n",
    "    test /= 255.\n",
    "    return train, train_labels, test, test_labels, [28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_mnist_dataset():\n",
    "    import torchvision.datasets as datasets\n",
    "\n",
    "    mnist_train = datasets.FashionMNIST(root='./data/fashion-mnist', train=True, download=True, transform=None)\n",
    "    mnist_test = datasets.FashionMNIST(root='./data/fashion-mnist', train=False, download=True, transform=None)\n",
    "    test_labels = np.array([mnist_test[i][1].numpy() for i in range(len(mnist_test))], dtype=np.int)\n",
    "    train_labels = np.array([mnist_train[i][1].numpy() for i in range(len(mnist_train))], dtype=np.int)\n",
    "    test = np.array([np.asarray(mnist_test[i][0]).reshape(28*28) for i in range(len(mnist_test))], dtype=np.float)\n",
    "    train = np.array([np.asarray(mnist_train[i][0]).reshape(28*28) for i in range(len(mnist_train))], dtype=np.float)\n",
    "    train /= 255.  # normalize data to be in range [0,1]\n",
    "    test /= 255.\n",
    "    return train, train_labels, test, test_labels, [28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emnist_dataset():\n",
    "    import torchvision.datasets as datasets\n",
    "\n",
    "    mnist_train = datasets.EMNIST(root='./data/emnist', split='balanced', train=True, download=True, transform=None)\n",
    "    mnist_test = datasets.EMNIST(root='./data/emnist', split='balanced', train=False, download=True, transform=None)\n",
    "    test_labels = np.array([mnist_test[i][1].numpy() for i in range(len(mnist_test))], dtype=np.int)\n",
    "    train_labels = np.array([mnist_train[i][1].numpy() for i in range(len(mnist_train))], dtype=np.int)\n",
    "    test = np.array([np.asarray(mnist_test[i][0]).reshape(28*28) for i in range(len(mnist_test))], dtype=np.float)\n",
    "    train = np.array([np.asarray(mnist_train[i][0]).reshape(28*28) for i in range(len(mnist_train))], dtype=np.float)\n",
    "    train /= 255.  # normalize data to be in range [0,1]\n",
    "    test /= 255.\n",
    "    return train, train_labels, test, test_labels, [28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_dataset():\n",
    "    import torchvision.datasets as datasets\n",
    "\n",
    "    cifar_train = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=None)\n",
    "    cifar_test = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=None)\n",
    "    test_labels = np.array([cifar_test[i][1] for i in range(len(cifar_test))], dtype=np.int)\n",
    "    train_labels = np.array([cifar_train[i][1] for i in range(len(cifar_train))], dtype=np.int)\n",
    "    test = np.array([np.asarray(cifar_test[i][0].convert('F')).reshape(32*32) for i in range(len(cifar_test))], dtype=np.float)\n",
    "    train = np.array([np.asarray(cifar_train[i][0].convert('F')).reshape(32*32) for i in range(len(cifar_train))], dtype=np.float)\n",
    "    train /= 255.  # normalize data to be in range [0,1]\n",
    "    test /= 255.\n",
    "    return train, train_labels, test, test_labels, [32, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Investigate Dataset using Outlier Detection\n",
    "\n",
    "There are two tasks:\n",
    "\n",
    "1. Implement a version of the rbf kernel with $\\exp(\\frac{-\\|x-y\\|^2}{param})$\n",
    "2. Implement the one-class SVM for the special case of $\\nu = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, train_labels, test, test_labels, im_size = load_mnist_dataset()\n",
    "train, train_labels, test, test_labels, im_size = load_fashion_mnist_dataset()\n",
    "# train, train_labels, test, test_labels, im_size = load_emnist_dataset()\n",
    "# train, train_labels, test, test_labels, im_size = load_cifar10_dataset()\n",
    "print('Train dataset size: ', train.shape)\n",
    "print('Test dataset size: ', test.shape)\n",
    "print('Num classes: ', np.unique(train_labels).size)\n",
    "print('Image size: ', im_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rbf_kernel(X, Y=None, param=1.):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_class_svm_nu_one(kernel):\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. crude check of your kernel implementation\n",
    "K = calc_rbf_kernel(train[:100, :], param=10.)\n",
    "\n",
    "# 1.a. check if eigenvalues are all non-negative (=positive semi-definite matrix)\n",
    "w, _ = np.linalg.eig(K)\n",
    "assert(np.all(w) >= 0.0)\n",
    "\n",
    "# 1.b. check if there are significant abberations from the sklearn implementation\n",
    "K_check = metrics.pairwise.rbf_kernel(train[:100, :], gamma=1./10.)\n",
    "assert(np.max(np.abs(K-K_check)) < 1e-03)\n",
    "\n",
    "# 2. crude check of your special case one-class SVM implementation\n",
    "outlier_scores = one_class_svm_nu_one(0.1*np.random.randn(4,4) + np.eye(4))\n",
    "assert(outlier_scores.size == 4)\n",
    "assert(outlier_scores.shape[0] == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS = 1\n",
    "NUM_TRAIN_SAMPLES = 200\n",
    "RBF_PARAM = 1.\n",
    "\n",
    "class_inds = np.where(train_labels == CLASS)[0]\n",
    "train_inds = np.random.permutation(inds.size)[:NUM_TRAIN_SAMPLES]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2, 5, 1)\n",
    "plt.title('Mean', fontsize=16)\n",
    "class_mean = np.mean(train[class_inds, :], axis=0)\n",
    "plt.imshow(class_mean.reshape(im_size[0], im_size[1]), cmap='binary')\n",
    "plt.xticks([], [], fontsize=14)\n",
    "plt.yticks([], [], fontsize=14)\n",
    "\n",
    "plt.subplot(2, 5, 2)\n",
    "plt.title('Median', fontsize=16)\n",
    "class_median = np.median(train[class_inds, :], axis=0)\n",
    "plt.imshow(class_median.reshape(im_size[0], im_size[1]), cmap='binary')\n",
    "plt.xticks([], [], fontsize=14)\n",
    "plt.yticks([], [], fontsize=14)\n",
    "\n",
    "kernel = calc_rbf_kernel(train[class_inds[train_inds], :], train[class_inds, :], param=RBF_PARAM)\n",
    "print(kernel.shape)\n",
    "outlier_scores = one_class_svm_nu_one(kernel)\n",
    "outlier_ranking = np.argsort(-outlier_scores)\n",
    "print(outlier_scores.shape)\n",
    "print('Memory footprint of your RBF matrix ({0}x{1} elements): {2}MB'.format(\n",
    "    kernel.shape[0], kernel.shape[1], np.int(kernel.size*kernel.itemsize / (1024.*1024.))))\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(2, 5, 3+i)\n",
    "    plt.title('Nominal {0}'.format(i+1), fontsize=16)\n",
    "    plt.imshow(train[class_inds[outlier_ranking[i]], :].reshape(im_size[0], im_size[1]), cmap='binary')\n",
    "    plt.xticks([], [], fontsize=14)\n",
    "    plt.yticks([], [], fontsize=14)\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(2, 5, 6+i)\n",
    "    plt.title('Outlier {0}'.format(i+1), fontsize=16)\n",
    "    plt.imshow(train[class_inds[outlier_ranking[-(i+1)]], :].reshape(im_size[0], im_size[1]), cmap='binary')\n",
    "    plt.xticks([], [], fontsize=14)\n",
    "    plt.yticks([], [], fontsize=14)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(outlier_ranking.size), outlier_scores[outlier_ranking], '.-r')\n",
    "plt.grid(True)\n",
    "plt.xlabel('sorted samples')\n",
    "plt.ylabel('outlier score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Image Classification using Histogram Intersection Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, train_labels, test, test_labels, im_size = load_mnist_dataset()\n",
    "# train, train_labels, test, test_labels, im_size = load_fashion_mnist_dataset()\n",
    "# train, train_labels, test, test_labels, im_size = load_emnist_dataset()\n",
    "train, train_labels, test, test_labels, im_size = load_cifar10_dataset()\n",
    "print('Train dataset size: ', train.shape)\n",
    "print('Test dataset size: ', test.shape)\n",
    "print('Num classes: ', np.unique(train_labels).size)\n",
    "print('Image size: ', im_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hist_intersect_kernel(X, Y=None):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    return -1\n",
    "\n",
    "K = calc_hist_intersect_kernel(train[:100, :], train[:10, :])\n",
    "assert(K.shape[0] == 100 and K.shape[1] == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_histogram_features(X, bins=10, normalize=False):\n",
    "    num_samples, num_features = X.shape\n",
    "    return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CLASS = 2  # 6: Shirt\n",
    "NEG_CLASS = 9  # 0: T-Shirt/Top\n",
    "\n",
    "BINS = 20\n",
    "NORMALIZE = True\n",
    "\n",
    "NUM_TRAIN_SAMPLES = 200\n",
    "NUM_TEST_SAMPLES = 200\n",
    "\n",
    "pos_class_inds = np.where(train_labels == POS_CLASS)[0]\n",
    "neg_class_inds = np.where(train_labels == NEG_CLASS)[0]\n",
    "class_inds = np.concatenate((pos_class_inds, neg_class_inds))\n",
    "lbl = train_labels[class_inds]\n",
    "lbl[lbl == NEG_CLASS] = -1\n",
    "lbl[lbl == POS_CLASS] = +1\n",
    "\n",
    "# split in train and test chunks\n",
    "rand_inds = np.random.permutation(class_inds.size)\n",
    "train_inds = rand_inds[:NUM_TRAIN_SAMPLES]\n",
    "test_inds = rand_inds[NUM_TRAIN_SAMPLES:]\n",
    "test_inds = test_inds[:NUM_TEST_SAMPLES]\n",
    "print('There are {0} positive samples and {1} negative samples in the dataset.'.format(\n",
    "    np.sum(lbl[rand_inds[:NUM_TRAIN_SAMPLES+NUM_TEST_SAMPLES]] == +1), \n",
    "    np.sum(lbl[rand_inds[:NUM_TRAIN_SAMPLES+NUM_TEST_SAMPLES]] == -1)))\n",
    "\n",
    "\n",
    "# calculate the histogram features and the corresponding train/test kernels\n",
    "transf_train = calc_histogram_features(train[class_inds, :], bins=BINS, normalize=NORMALIZE)\n",
    "K1 = calc_hist_intersect_kernel(transf_train[train_inds, :])\n",
    "K2 = calc_hist_intersect_kernel(transf_train[train_inds, :], transf_train[test_inds, :])\n",
    "\n",
    "# train the SVM using our histogram kernel\n",
    "classifier = svm.SVC(kernel='precomputed')\n",
    "classifier.fit(K1, lbl[train_inds])\n",
    "pred = classifier.decision_function(K2.T)\n",
    "# pred = classifier.predict(K2.T)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(lbl[test_inds], pred, pos_label=1)\n",
    "auc_hist_kernel = metrics.auc(fpr, tpr)\n",
    "\n",
    "# train the SVM using standard RBF kernels\n",
    "classifier = svm.SVC(kernel='rbf', gamma='auto')\n",
    "classifier.fit(train[class_inds[train_inds], :].copy(), lbl[train_inds])\n",
    "pred = classifier.decision_function(train[class_inds[test_inds], :])\n",
    "# pred = classifier.predict(train[class_inds[test_inds], :].copy())\n",
    "fpr, tpr, thresholds = metrics.roc_curve(lbl[test_inds], pred, pos_label=1)\n",
    "auc_rbf_kernel = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('AUC RBF kernel: ', auc_rbf_kernel)\n",
    "print('AUC histogram intersection kernel: ', auc_hist_kernel)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Mean Class-wise Histograms')\n",
    "mean_hist_neg = np.mean(transf_train[lbl==-1, :], axis=0)\n",
    "mean_hist_pos = np.mean(transf_train[lbl==+1, :], axis=0)\n",
    "hist_intersect = np.min((mean_hist_neg, mean_hist_pos), axis=0)\n",
    "\n",
    "plt.bar(np.arange(BINS), mean_hist_neg, alpha=0.8)\n",
    "plt.bar(np.arange(BINS)+0.2, mean_hist_pos, alpha=0.8)\n",
    "plt.plot(np.arange(BINS), hist_intersect, '.-r')\n",
    "plt.grid(True)\n",
    "plt.legend(['intersect', '-', '+'], loc=2)\n",
    "plt.xlabel('bins')\n",
    "plt.ylabel('mean occurance')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Histogram Example')\n",
    "hist_neg_exm = transf_train[0, :]\n",
    "hist_pos_exm = transf_train[-1, :]\n",
    "hist_intersect = np.min((hist_neg_exm, hist_pos_exm), axis=0)\n",
    "\n",
    "plt.bar(np.arange(BINS), hist_neg_exm, alpha=0.8)\n",
    "plt.bar(np.arange(BINS)+0.2, hist_pos_exm, alpha=0.8)\n",
    "plt.plot(np.arange(BINS), hist_intersect, '.-r')\n",
    "plt.grid(True)\n",
    "plt.legend(['intersect', 'exm (lbl={0})'.format(lbl[0]), 'exm (lbl={0})'.format(lbl[-1])], loc=2)\n",
    "plt.xlabel('bins')\n",
    "plt.ylabel('mean occurance')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Multiple Kernel Learning\n",
    "\n",
    "Your tasks:\n",
    "1. Finish the 'normalize_kernel' function (=normalize datapoints in feature space) \n",
    "2. Implement 'center_kernel' (=center datapoints in feature space)\n",
    "3. Implement 'combine_kernels' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kernel(K):\n",
    "    # A kernel K is normalized, iff K_ii = 1 \\forall i\n",
    "    return -1\n",
    "    \n",
    "def center_kernel(K):\n",
    "    # Mean free in feature space\n",
    "    return -1\n",
    "    \n",
    "def combine_kernels(kernels, mix):\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mkl(kernels, lbls, pnorm=2., precision=1e-9, max_iter=100):\n",
    "    iter = 0\n",
    "    dm = np.ones(len(kernels), dtype=np.float) / len(kernels)\n",
    "    lastsol = np.zeros(len(kernels))\n",
    "    while np.max(np.abs(lastsol-dm)) > precision and iter < max_iter:\n",
    "        # train svm\n",
    "        combined_kernel = combine_kernels(kernels, dm)\n",
    "        classifier = svm.SVC(kernel='precomputed')\n",
    "        classifier.fit(combined_kernel, lbls)\n",
    "\n",
    "        # calculate new kernel mixing coefficients\n",
    "        lastsol = dm.copy()\n",
    "        alphas = np.abs(classifier.dual_coef_)\n",
    "\n",
    "        norm_w_sq_m = np.zeros((len(kernels), 1))\n",
    "        res = lbls.dot(lbls.T)*alphas.dot(alphas.T)\n",
    "        for l in range(len(kernels)):\n",
    "            norm_w_sq_m[l] = np.sum(dm[l]*dm[l]*res*kernels[l])\n",
    "\n",
    "        sum_norm_w = np.sum(np.power(norm_w_sq_m, pnorm/(pnorm+1.0)))\n",
    "        sum_norm_w = np.power(sum_norm_w, 1.0/pnorm)\n",
    "\n",
    "        dm = np.power(norm_w_sq_m, 1.0/(pnorm+1.0))/sum_norm_w\n",
    "        dm_norm = np.sum(np.power(abs(dm), pnorm))\n",
    "        dm_norm = np.power(dm_norm, 1.0/pnorm)\n",
    "        iter+=1\n",
    "\n",
    "    print('Num iterations = {0}.'.format(iter))\n",
    "    print('New mixing coefficients:')\n",
    "    print(dm)\n",
    "    print(dm_norm)\n",
    "    return dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_NORM = 2.2\n",
    "MKL_BINS = [2, 5, 10, 200]\n",
    "MKL_RBF = [0.001, 1.0, train[0].size]\n",
    "\n",
    "all_inds = np.concatenate((train_inds, test_inds)) \n",
    "\n",
    "train_kernels = list()\n",
    "test_kernels = list()\n",
    "for num_bins in MKL_BINS:\n",
    "    transf_train = calc_histogram_features(train[class_inds, :], bins=num_bins, normalize=NORMALIZE)\n",
    "    kernel = calc_hist_intersect_kernel(transf_train[all_inds, :])\n",
    "    kernel = center_kernel(kernel)\n",
    "    kernel = normalize_kernel(kernel)\n",
    "    train_kernels.append(kernel[:train_inds.size, :train_inds.size])\n",
    "    test_kernels.append(kernel[:train_inds.size, train_inds.size:])\n",
    "for param in MKL_RBF:\n",
    "    kernel = calc_rbf_kernel(train[class_inds[all_inds], :], param=param)\n",
    "    kernel = center_kernel(kernel)\n",
    "    kernel = normalize_kernel(kernel)\n",
    "    train_kernels.append(kernel[:train_inds.size, :train_inds.size])\n",
    "    test_kernels.append(kernel[:train_inds.size, train_inds.size:])\n",
    "\n",
    "# 1. learn kernel weights\n",
    "mix = fit_mkl(train_kernels, lbl[train_inds], pnorm=P_NORM)\n",
    "combined_kernel = combine_kernels(train_kernels, mix)\n",
    "classifier = svm.SVC(kernel='precomputed')\n",
    "classifier.fit(combined_kernel, lbl[train_inds])\n",
    "combined_kernel = combine_kernels(test_kernels, mix)\n",
    "pred = classifier.decision_function(combined_kernel.T)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(lbl[test_inds], pred, pos_label=1)\n",
    "auc_mkl_kernel = metrics.auc(fpr, tpr)\n",
    "\n",
    "# 2. use sum kernel (=inf norm) \n",
    "mix_uniform = np.ones(len(train_kernels), dtype=np.float) / len(combined_kernel)\n",
    "combined_kernel = combine_kernels(train_kernels, mix_uniform)\n",
    "classifier = svm.SVC(kernel='precomputed')\n",
    "classifier.fit(combined_kernel, lbl[train_inds])\n",
    "combined_kernel = combine_kernels(test_kernels, mix_uniform)\n",
    "pred = classifier.decision_function(combined_kernel.T)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(lbl[test_inds], pred, pos_label=1)\n",
    "auc_sum_kernel = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('AUC RBF kernel: ', auc_rbf_kernel)\n",
    "print('AUC histogram intersection kernel: ', auc_hist_kernel)\n",
    "print('AUC combined kernel (sum): ', auc_sum_kernel)\n",
    "print('AUC combined kernel (p={0}): '.format(P_NORM), auc_mkl_kernel)\n",
    "\n",
    "plt.bar(np.arange(mix.size)[:len(MKL_BINS)], mix[:len(MKL_BINS), 0])\n",
    "plt.bar(np.arange(mix.size)[len(MKL_BINS):], mix[len(MKL_BINS):, 0])\n",
    "plt.xticks(np.arange(mix.size))\n",
    "plt.grid(True)\n",
    "plt.xlabel('kernel')\n",
    "plt.ylabel('weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Kernel Approximations\n",
    "\n",
    "It is often more natural to design useful kernels rather than explicitly designing feature maps. However, kernel matrices grow quadratic with the number of samples which makes them unsuitable for large-scale problems. To overcome this problem, approximations such as 'Random Kitchen Sinks' calculate a feature map that emulates certain kernels (e.g. Gaussian and RBF kernels). In this part, we examine the runtime and convergence behaviour of random Fourier features. \n",
    "\n",
    "Your single task is to implement the 'calc_rbf_features' function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, train_labels, test, test_labels, im_size = load_mnist_dataset()\n",
    "# train, train_labels, test, test_labels, im_size = load_fashion_mnist_dataset()\n",
    "# train, train_labels, test, test_labels, im_size = load_emnist_dataset()\n",
    "train, train_labels, test, test_labels, im_size = load_cifar10_dataset()\n",
    "print('Train dataset size: ', train.shape)\n",
    "print('Test dataset size: ', test.shape)\n",
    "print('Num classes: ', np.unique(train_labels).size)\n",
    "print('Image size: ', im_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Bochner) Assume that $K$ is a positive definite translation-invariant (real) kernel (i.e. $K(x,y) = K(x-y)$) then\n",
    "it can be represented as the Fourier transform of a probability distribution:\n",
    "\n",
    "$K(\\Delta) = \\int \\exp(iw^T\\Delta)p(w)dw = \\mathbb{E}_{w\\sim p}[\\exp(iw^T\\Delta)] = \\mathbb{E}_{w\\sim p}[\\cos(w^T\\Delta)] = \\mathbb{E}_{w\\sim p}[\\cos(w^T(x-y))] = \\mathbb{E}_{w\\sim p}[\\cos(w^Tx)\\cos(w^Ty) + \\sin(w^Tx)\\sin(w^Ty)]$\n",
    "\n",
    "Hence, we can approximate $K$ by using $\\sin(w^Tx)$ and $\\cos(w^Tx)$ features with randomly sampled $w \\sim p$: \n",
    "\n",
    "$K(\\Delta) \\approx \\frac{1}{D}\\sum_{i=1}^D[\\cos(w_i^Tx)\\cos(w_i^Ty) + \\sin(w_i^Tx)\\sin(w_i^Ty)] =^! \\phi(x)^T \\phi(y)$\n",
    "\n",
    "1. For isotropic Gaussian kernel (which is translation invariant) with $\\sigma=1$, $K(\\Delta) = \\exp(-\\|\\Delta\\|^2/2)$ the corresponding probability distribution is $p(w) = (2\\pi)^{-\\frac{|\\mathcal{X}|}{2}} \\exp{\\frac{-\\|w\\|^2}{2}}$. Numpy's build-in normal distribtion (setting $\\mu=0$ and $\\sigma=1$) reads $p(x) = (2\\pi)^{-\\frac{1}{2}} \\exp{\\frac{-\\|x\\|^2}{2}}$ (for a single feature). Use numpy's 'normal' function to sample $w$.\n",
    "\n",
    "2. We do not want to sample $\\cos$ and $\\sin$. Instead, we use a variant that samples an offset $b \\sim unif(0,2\\pi)$ and only uses a single $\\cos$ twice, i.e. feature $i=1,\\ldots,D$ becomes $\\sqrt{2}\\cos(w_i^Tx + b_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rbf_features(X, n_features=50):\n",
    "    return -1\n",
    "    \n",
    "feats = calc_rbf_features(np.random.normal(size=(10, 20)), n_features=5)\n",
    "assert(feats.size == 10*5)\n",
    "assert(feats.shape[0] == 10 and feats.shape[1] == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Runtime and convergence behaviour for increasing number of random Fourier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CLASS = 6  # 6: Shirt\n",
    "NEG_CLASS = 0  # 0: T-Shirt/Top\n",
    "NUM_SAMPLES = 1000\n",
    "NUM_FEATURES = [5, 10, 20, 40, 80, 160, 320, 640, 1280, 12800]\n",
    "\n",
    "pos_class_inds = np.where(train_labels == POS_CLASS)[0]\n",
    "neg_class_inds = np.where(train_labels == NEG_CLASS)[0]\n",
    "sample_inds = np.concatenate((pos_class_inds, neg_class_inds))\n",
    "lbls = train_labels[sample_inds]\n",
    "lbls[lbls == NEG_CLASS] = -1\n",
    "lbls[lbls == POS_CLASS] = +1\n",
    "rand_inds = np.random.permutation(sample_inds.size)\n",
    "sample_inds = sample_inds[rand_inds[:NUM_SAMPLES]]\n",
    "lbls = lbls[rand_inds[:NUM_SAMPLES]]\n",
    "\n",
    "kernel = calc_rbf_kernel(train[sample_inds, :], param=1./1.)\n",
    "times = np.zeros((len(NUM_FEATURES), 2))\n",
    "approx_error = np.zeros(len(NUM_FEATURES))\n",
    "for i in range(len(NUM_FEATURES)):\n",
    "    rbf_features = calc_rbf_features(train[sample_inds, :], n_features=NUM_FEATURES[i])\n",
    "\n",
    "    approx_kernel = rbf_features.dot(rbf_features.T)\n",
    "    classifier = svm.LinearSVC(loss='hinge')\n",
    "    start = time.time()\n",
    "    classifier.fit(rbf_features, lbls)\n",
    "    end = time.time()\n",
    "    times[i, 0] = end - start\n",
    "\n",
    "    classifier = svm.SVC(kernel='precomputed')\n",
    "    start = time.time()\n",
    "    classifier.fit(kernel, lbls)\n",
    "    end = time.time()\n",
    "    times[i, 1] = end - start\n",
    "\n",
    "    approx_error[i] = np.sum(np.abs(approx_kernel - kernel) / kernel.size)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(NUM_FEATURES)), approx_error, '-.b')\n",
    "plt.xticks(np.arange(len(NUM_FEATURES)+1), NUM_FEATURES)\n",
    "plt.grid(True)\n",
    "plt.xlabel('# random features')\n",
    "plt.ylabel('reconstruction error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(len(NUM_FEATURES)), times[:, 0], '-.b')\n",
    "plt.plot(np.arange(len(NUM_FEATURES)), times[:, 1], '-.r')\n",
    "plt.xticks(np.arange(len(NUM_FEATURES)+1), NUM_FEATURES)\n",
    "plt.grid(True)\n",
    "plt.legend(['random', 'rbf'], loc=2)\n",
    "plt.xlabel('# random features')\n",
    "plt.ylabel('processing time [sec]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Runtime and convergence behaviour for increasing number of samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = [100, 200, 400, 800, 1600, 3200, 6400]\n",
    "NUM_FEATURES = 640\n",
    "\n",
    "mem = np.zeros((len(NUM_SAMPLES), 2))\n",
    "times = np.zeros((len(NUM_SAMPLES), 2))\n",
    "approx_error = np.zeros(len(NUM_SAMPLES))\n",
    "for i in range(len(NUM_SAMPLES)):\n",
    "    sample_inds = np.concatenate((pos_class_inds, neg_class_inds))\n",
    "    lbls = train_labels[sample_inds]\n",
    "    lbls[lbls == NEG_CLASS] = -1\n",
    "    lbls[lbls == POS_CLASS] = +1\n",
    "    rand_inds = np.random.permutation(sample_inds.size)\n",
    "    sample_inds = sample_inds[rand_inds[:NUM_SAMPLES[i]]]\n",
    "    lbls = lbls[rand_inds[:NUM_SAMPLES[i]]]\n",
    "\n",
    "    rbf_features = calc_rbf_features(train[sample_inds, :], n_features=NUM_FEATURES)\n",
    "    kernel = calc_rbf_kernel(train[sample_inds, :], param=1./1.)\n",
    "\n",
    "    approx_kernel = rbf_features.dot(rbf_features.T)\n",
    "    classifier = svm.LinearSVC(loss='hinge')\n",
    "    start = time.time()\n",
    "    classifier.fit(rbf_features, lbls)\n",
    "    end = time.time()\n",
    "    times[i, 0] = end - start\n",
    "\n",
    "    classifier = svm.SVC(kernel='precomputed')\n",
    "    start = time.time()\n",
    "    classifier.fit(kernel, lbls)\n",
    "    end = time.time()\n",
    "    times[i, 1] = end - start\n",
    "\n",
    "    approx_error[i] = np.sum(np.abs(approx_kernel - kernel) / kernel.size)\n",
    "    mem[i, 0] = np.int(sample_inds.size*NUM_FEATURES*train.itemsize / (1024.*1024.))\n",
    "    mem[i, 1] = np.int(kernel.size*kernel.itemsize / (1024.*1024.))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(np.arange(len(NUM_SAMPLES)), approx_error, '-.b')\n",
    "plt.xticks(np.arange(len(NUM_SAMPLES)+1), NUM_SAMPLES)\n",
    "plt.grid(True)\n",
    "plt.xlabel('# samples')\n",
    "plt.ylabel('reconstruction error')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(np.arange(len(NUM_SAMPLES)), times[:, 0], '-.b')\n",
    "plt.plot(np.arange(len(NUM_SAMPLES)), times[:, 1], '-.r')\n",
    "plt.xticks(np.arange(len(NUM_SAMPLES)+1), NUM_SAMPLES)\n",
    "plt.grid(True)\n",
    "plt.legend(['random', 'rbf'], loc=2)\n",
    "plt.xlabel('# samples')\n",
    "plt.ylabel('processing time [sec]')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(np.arange(len(NUM_SAMPLES)), mem[:, 0], '-.b')\n",
    "plt.plot(np.arange(len(NUM_SAMPLES)), mem[:, 1], '-.r')\n",
    "plt.xticks(np.arange(len(NUM_SAMPLES)+1), NUM_SAMPLES)\n",
    "plt.grid(True)\n",
    "plt.legend(['random', 'rbf'], loc=2)\n",
    "plt.xlabel('# samples')\n",
    "plt.ylabel('memory footprint [MB]')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
