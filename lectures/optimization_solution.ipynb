{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Exercise: _Optimization_\n",
    "***\n",
    "\n",
    "We will implement various optimization algorithms and examine their performance for various tasks.\n",
    "\n",
    "\n",
    "1. First-order, smooth optimization using gradient descent \n",
    "    - Implement basic gradient descent solver\n",
    "    - Implement gradient descent with armijo backtracking\n",
    "2. Smooth One-class SVM\n",
    "    - Implement hinge- and Huber loss functions\n",
    "    - Implement objective and derivative of the smooth one-class svm\n",
    "    - Use check_grad to verify the implementation\n",
    "3. First-order, non-smooth optimization using sub-gradient descent\n",
    "    - Implement objective and derivative of $\\ell_p$-norm regularized one-class svm\n",
    "4. Utilizing Available QP Solver Packages: CVXOPT\n",
    "    - Use cvxopt qp solver to solve the primal one-class svm optimization problem\n",
    "5. Utilizing Available Solver Packages: SciPy's Optimization Suite\n",
    "    - Apply scipy's _minimize_ function on your implementation of the objective function of the smooth one-class svm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from scipy.optimize import check_grad, minimize\n",
    "\n",
    "import numpy as np\n",
    "import cvxopt as cvx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First-order, smooth optimization using gradient descent \n",
    "\n",
    "In this first part, we want use various variants of gradient descent for continuous and \n",
    "smooth optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A well-known continuous, convex, smooth method is l2-norm regularized logistic regression.\n",
    "Which has the following objective function:\n",
    "\n",
    "$f(w) = \\frac{\\lambda}{2} \\|w\\|^2 + \\sum_{i=1}^n \\log(1+\\exp(-y_i\\langle w, x_i \\rangle))$\n",
    "\n",
    "In order to apply gradient descent, we will further need the first derivative:\n",
    "\n",
    "$f'(w) = \\lambda w + \\sum_{i=1}^n \\frac{-y_i}{1+\\exp(y_i(\\langle w, x_i \\rangle))}x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_l2_logistic_regression(w, X, y, param):\n",
    "    w = w.reshape(w.size, 1)\n",
    "    t1 = 1. + np.exp(-y * w.T.dot(X).T)\n",
    "    f = param/2.*w.T.dot(w) + np.sum(np.log(t1))\n",
    "    return f[0,0]\n",
    "\n",
    "def grad_l2_logistic_regression(w, X, y, param):\n",
    "    w = w.reshape(w.size, 1)\n",
    "    t2 = 1. + np.exp(y * w.T.dot(X).T)\n",
    "    grad = param*w + (-y/t2).T.dot(X.T).T\n",
    "    return grad.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMin0(fun, grad, w, alpha, max_evals=10, eps=1e-2, verbosity=1):\n",
    "    f = fun(w)\n",
    "    g = grad(w)\n",
    "    evals = 1\n",
    "    while evals < max_evals:\n",
    "        w = w - alpha * g\n",
    "        f = fun(w)\n",
    "        g = grad(w)\n",
    "        evals += 1\n",
    "        opt_cond = np.linalg.norm(g, ord=np.inf)\n",
    "        if verbosity > 0:\n",
    "            print('{0} {1:5.5f} {2:5.5f} {3:5.5f}'.format(evals, alpha, f, opt_cond))\n",
    "        if opt_cond < eps:\n",
    "            break\n",
    "    print('{0} {1:5.5f} {2:5.5f} {3:5.5f}'.format(evals, alpha, f, opt_cond))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let us generate some small data set to try out our optimization schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate some test data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 100)\n",
    "w = np.random.randn(10, 1)\n",
    "reg_y = w.T.dot(X)\n",
    "median = np.median(reg_y)\n",
    "y = -np.ones((reg_y.size, 1), dtype=np.int)\n",
    "y[reg_y.ravel() >= median] = +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a look at the most basic gradient descent method we can think of and \n",
    "start playing with the step-size $\\alpha$. Try some, e.g.\n",
    "- $\\alpha=1.0$ \n",
    "- $\\alpha=1e-6$\n",
    "- $\\alpha=0.001$\n",
    "\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.334026460825949e-06\n",
      "1000 0.00100 23.10870 0.21163\n"
     ]
    }
   ],
   "source": [
    "fun = partial(fun_l2_logistic_regression, X=X, y=y, param=1.)\n",
    "grad = partial(grad_l2_logistic_regression, X=X, y=y, param=1.)\n",
    "\n",
    "print(check_grad(fun, grad, 0.0*np.random.randn(10)))\n",
    "\n",
    "wstar = findMin0(fun, grad, 0.0*np.random.randn(10), 0.001, max_evals=1000, eps=1e-8, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to tweak the $\\alpha$'s for every single optimization problem. This is where\n",
    "line search steps in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMinBT(fun, grad, w, alpha, gamma, max_evals=10, eps=1e-2, verbosity=1):\n",
    "    f = fun(w)\n",
    "    g = grad(w)\n",
    "    evals = 1\n",
    "    while evals < max_evals:\n",
    "        while fun(w - alpha * g) > f - gamma*alpha*g.dot(g):\n",
    "            alpha /= 2.\n",
    "        w = w - alpha * g\n",
    "        f = fun(w)\n",
    "        g = grad(w)            \n",
    "        evals += 1\n",
    "        opt_cond = np.linalg.norm(g, ord=np.inf)\n",
    "        if verbosity > 0:\n",
    "            print('{0} {1:5.5f} {2:5.5f} {3:5.5f}'.format(evals, alpha, f, opt_cond))\n",
    "        if opt_cond < eps:\n",
    "            break\n",
    "    print('{0} {1:5.5f} {2:5.5f} {3:5.5f}'.format(evals, alpha, f, opt_cond))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.334026460825949e-06\n",
      "28 0.06250 23.08338 0.00862\n"
     ]
    }
   ],
   "source": [
    "fun = partial(fun_l2_logistic_regression, X=X, y=y, param=1.)\n",
    "grad = partial(grad_l2_logistic_regression, X=X, y=y, param=1.)\n",
    "\n",
    "print(check_grad(fun, grad, 0.0*np.random.randn(10)))\n",
    "wstar = findMinBT(fun, grad, 0.0*np.random.randn(10), 1., 0.001, max_evals=100, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More elaborate optimization methods (e.g. Newton descent) will use second-order information in order to find a better step-length. We will come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Smooth One-class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an anomaly detection workshop, we want to train some anomaly detectors.\n",
    "So, here is our one-class SVM primal problem again:\n",
    "\n",
    "$\\min_{w,\\rho,\\xi} \\frac{1}{2}\\|w\\|^2 - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n \\xi_i$\n",
    "\n",
    "subject to the following constraints:\n",
    "\n",
    "$\\xi_i \\geq 0\\;, \\quad \\langle w, x_i \\rangle \\geq \\rho - \\xi_i \\; , \\quad \\forall \\; i$\n",
    "\n",
    "This OP is unfortunately neither smooth nor unconstrained. So, lets change this.\n",
    "1. We will get rid of the constraints by re-formulating them \n",
    "$\\xi_i \\geq 0\\;, \\quad \\langle w, x_i \\rangle \\geq \\rho - \\xi_i \\; \\Rightarrow \n",
    "\\xi_i \\geq 0\\;, \\quad \\xi_i \\geq \\rho - \\langle w, x_i \\rangle$\n",
    "Since we minimize the objective, the RHS will hold with equality. Hence we can\n",
    "replace $\\xi_i$ in the objective with the RHS. However, we need to take care also of the\n",
    "LHS which states that if LHS is smaller than $0$ the value should stay $0$. This can be achieved\n",
    "by taking a $max(0, RHS)$. Hence, we land at the following _unconstrained_ problem:\n",
    "\n",
    "    $\\min_{w,\\rho} \\frac{1}{2}\\|w\\|^2 - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n max(0,\\rho - \\langle w, x_i \\rangle)$,\n",
    "\n",
    "    which can be written in terms of a general loss function $\\ell$ as\n",
    "\n",
    "    $\\min_{w,\\rho} \\frac{1}{2}\\|w\\|^2 - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n \\ell(\\rho - \\langle w, x_i \\rangle)$.\n",
    "\n",
    "\n",
    "    This is now unconstrained but still not smooth as the max (which is BTW called the hinge-loss) introduces some non-smoothness into the problem and gradient descent solvers can not readily applied. So, lets make it differentiable by approximation.\n",
    "    \n",
    "    \n",
    "2. Approximating the hinge-loss by differentiable Huber-loss\n",
    "\n",
    "    $\\ell_{\\Delta,\\epsilon}(x) := \n",
    "        \\left\\{\\begin{array}{lr}\n",
    "         x -\\Delta, & \\text{for } x \\geq \\Delta + \\epsilon\\\\\n",
    "        \\frac{(\\epsilon + x - \\Delta)^2}{4\\epsilon}, & \\text{for } \\Delta - \\epsilon\\leq x\\leq \\Delta + \\epsilon\\\\\n",
    "        0, & \\text{else}\n",
    "        \\end{array}\\right\\}$\n",
    "        \n",
    "    ..and the corresponding derivative is (I hope):\n",
    "   \n",
    "    $\\frac{\\partial}{\\partial x}\\ell_{\\Delta,\\epsilon}(x) := \n",
    "        \\left\\{\\begin{array}{lr}\n",
    "        1, & \\text{for } x \\geq \\Delta + \\epsilon\\\\\n",
    "        \\frac{(\\epsilon + x - \\Delta)}{2\\epsilon}, & \\text{for } \\Delta - \\epsilon\\leq x\\leq \\Delta + \\epsilon\\\\\n",
    "        0, & \\text{else}\n",
    "        \\end{array}\\right\\}$\n",
    "    \n",
    "    For our purposes, $\\Delta=0.0$ and $\\epsilon=0.5$ will suffice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Implement the hinge loss $\\ell(x) = \\max(0,x)$\n",
    "\n",
    "(2) Implement the Huber loss as defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(x):\n",
    "    l_x = np.zeros(x.size)\n",
    "    inds = np.argwhere(x > 0).ravel()\n",
    "    l_x[inds] = x[inds]\n",
    "    return l_x\n",
    "\n",
    "def huber_loss(x, delta, epsilon):\n",
    "    l_x = np.zeros(x.size)\n",
    "    \n",
    "    inds = np.argwhere(x >= delta + epsilon).ravel()\n",
    "    l_x[inds] = x[inds] - delta\n",
    "    \n",
    "    inds = np.argwhere(np.logical_and((delta - epsilon <= x), (x <= delta + epsilon))).ravel()\n",
    "    l_x[inds] = (epsilon+x[inds]-delta)*(epsilon+x[inds]-delta) / (4.*epsilon)\n",
    "    return l_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(-1, +1, 1000)\n",
    "\n",
    "plt.plot(xs, hinge_loss(xs), '-r', linewidth=2.0)\n",
    "plt.plot(xs, huber_loss(xs, 0.0, 0.5), '--b', linewidth=2.0)\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_smooth_ocsvm(var, X, nu, delta, epsilon):\n",
    "    rho = var[0]\n",
    "    w = var[1:]\n",
    "    w = w.reshape(w.size, 1)\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    d = X.shape[0]\n",
    "    \n",
    "    inner = (rho - w.T.dot(X)).ravel()\n",
    "    loss = np.zeros(n)\n",
    "    \n",
    "    inds = np.argwhere(inner >= delta + epsilon)\n",
    "    loss[inds] = inner[inds] - delta\n",
    "\n",
    "    inds = np.argwhere(np.logical_and((delta - epsilon <= inner), (inner <= delta + epsilon))).ravel()\n",
    "    loss[inds] = (epsilon + inner[inds]- delta)*(epsilon + inner[inds] -delta) / (4.*epsilon)\n",
    "    \n",
    "    f = 1./2.*w.T.dot(w) - rho + np.sum(loss) / (n*nu)\n",
    "    return f[0,0]\n",
    "\n",
    "def grad_smooth_ocsvm(var, X, nu, delta, epsilon):\n",
    "    rho = var[0]\n",
    "    w = var[1:]\n",
    "    w = w.reshape(w.size, 1)\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    d = X.shape[0]\n",
    "    \n",
    "    inner = (rho - w.T.dot(X)).ravel()\n",
    "    grad_loss_rho = np.zeros(n)\n",
    "    grad_loss_w = np.zeros((n,d))\n",
    "    \n",
    "    inds = np.argwhere(inner >= delta + epsilon).ravel()\n",
    "    grad_loss_rho[inds] = 1.\n",
    "    grad_loss_w[inds, :] = -X[:, inds].T\n",
    "\n",
    "    inds = np.argwhere(np.logical_and((delta - epsilon <= inner), (inner <= delta + epsilon))).ravel()\n",
    "    grad_loss_rho[inds] = (-delta + epsilon + inner[inds]) / (2.*epsilon) \n",
    "    grad_loss_w[inds, :] = ((-delta + epsilon + inner[inds]) / (2.*epsilon) * (-X[:, inds])).T\n",
    "    \n",
    "    grad = np.zeros(d+1)\n",
    "    grad[0] = -1 + np.sum(grad_loss_rho) / (n*nu)\n",
    "    grad[1:] = w.ravel() + np.sum(grad_loss_w, axis=0) / (n*nu)\n",
    "    return grad.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0391285241858912e-07\n",
      "362 1.00000 -0.04851 0.00010\n",
      "[-0.10373093  0.02240406  0.0648898   0.10672381 -0.05590757 -0.11514215\n",
      "  0.02419325 -0.00616242  0.22849043  0.02790996]\n",
      "[-0.10384652  0.02230459  0.06489625  0.10684012 -0.05600447 -0.1153056\n",
      "  0.02409386 -0.00616057  0.22864907  0.02785382]\n",
      "0.00033336020184623736\n"
     ]
    }
   ],
   "source": [
    "fun = partial(fun_smooth_ocsvm, X=X, nu=1.0, delta=0., epsilon=0.5)\n",
    "grad = partial(grad_smooth_ocsvm, X=X, nu=1.0, delta=0., epsilon=0.5)\n",
    "\n",
    "# First, check gradient vs numerical gradient.\n",
    "# This should give very small results.\n",
    "print(check_grad(fun, grad, np.random.randn(10+1)))\n",
    "\n",
    "xstar = findMinBT(fun, grad, 0.0*np.random.randn(10+1), 1., 0.0001, max_evals=1000, eps=1e-4, verbosity=0)\n",
    "wstar = xstar[1:]\n",
    "\n",
    "print(wstar)\n",
    "print(np.mean(X, axis=1))\n",
    "print(np.linalg.norm(wstar - np.mean(X, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. First-order, non-smooth optimization using sub-gradient descent \n",
    "\n",
    "Unfortunately, many interesting methods do contain a non-smooth part in their objective. Examples \n",
    "include support vector machines (SVMs), one-class support vector machines (OCSVM), and support vector data descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we gonna implement a version of the primal one-class SVM with a $\\ell_p$-norm \n",
    "regularizer. This will allow us to control the sparsity of the found solution vector:\n",
    "\n",
    "$\\min_{w,\\rho} \\|w\\|_p - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n max(0,\\rho - \\langle w, x_i \\rangle)$,\n",
    "\n",
    "The resulting optimization problem is unconstrained but non-smooth. We will use a subgradient descent solver \n",
    "for this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMinSG(fun, grad, x0, rate, max_evals=1000, eps=1e-2, step_method=1, verbosity=1):\n",
    "    dims = x0.size\n",
    "    x = x0\n",
    "    best_x = x\n",
    "    best_obj = np.float64(1e20)\n",
    "    obj_bak = -1e10\n",
    "    evals = 0\n",
    "    is_converged = False\n",
    "    while not is_converged and evals < max_evals:\n",
    "        obj = fun(x)\n",
    "        # this is subgradient, hence need to store the best solution so far\n",
    "        if best_obj >= obj:\n",
    "            best_x = x\n",
    "            best_obj = obj\n",
    "\n",
    "        # stop, if progress is too slow\n",
    "        if np.abs((obj-obj_bak)) < eps:\n",
    "            is_converged = True\n",
    "            continue\n",
    "        obj_bak = obj\n",
    "        \n",
    "        # gradient step for threshold\n",
    "        g = grad(x)\n",
    "        if step_method == 1:\n",
    "            # constant step\n",
    "            alpha = rate\n",
    "        elif step_method == 2:\n",
    "            # non-summable dimishing step size\n",
    "            alpha = rate / np.sqrt(np.float(evals+1.)) / np.linalg.norm(g)\n",
    "        else:\n",
    "            # const. step length\n",
    "            alpha = rate / np.linalg.norm(g)\n",
    "            \n",
    "        if verbosity > 0:\n",
    "            print('{0} {1:5.5f} {2:5.5f} {3:5.5f}'.format(evals, alpha, obj, np.abs((obj-obj_bak))))           \n",
    "\n",
    "        # update\n",
    "        x = x - alpha*g\n",
    "        evals += 1\n",
    "\n",
    "    print('{0} {1:5.5f} {2:5.5f} {3:5.5f}'.format(evals, alpha, obj, np.abs((obj-obj_bak))))           \n",
    "    return best_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4215589009424396e-06\n"
     ]
    }
   ],
   "source": [
    "def fun_lp_norm(w, p=2.):\n",
    "    pnorm = np.sum(np.abs(w)**p)**(1./p)    \n",
    "    return pnorm\n",
    "\n",
    "def grad_lp_norm(w, p=2.):\n",
    "    pnorm1 = np.sum(np.abs(w)**p)**((p-1.)/p)\n",
    "    grad_pnorm = (w*np.abs(w)**(p-2.)) / pnorm1\n",
    "    return grad_pnorm.ravel()\n",
    "\n",
    "fun = partial(fun_lp_norm, p=1.2)\n",
    "grad = partial(grad_lp_norm, p=1.2)\n",
    "print(check_grad(fun, grad, np.random.randn(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_lp_norm_ocsvm(var, X, p, nu):\n",
    "    feat, n = X.shape\n",
    "    w = var[1:]\n",
    "    rho = var[0]\n",
    "    pnorm = np.sum(np.abs(w)**p)**(1./p)    \n",
    "    slacks = rho - w.T.dot(X)\n",
    "    slacks[slacks < 0.] = 0.\n",
    "    return (pnorm - rho + np.sum(slacks) / (n*nu))\n",
    "\n",
    "def grad_lp_norm_ocsvm(var, X, p, nu):\n",
    "    feats, n = X.shape\n",
    "    w = var[1:]\n",
    "    rho = var[0]\n",
    "\n",
    "    pnorm1 = np.sum(np.abs(w)**p)**((p-1.)/p)\n",
    "    grad_pnorm = (w*np.abs(w)**(p-2.)) / pnorm1\n",
    "\n",
    "    slacks = rho - w.T.dot(X)\n",
    "    inds = np.argwhere(slacks >= 0.0)\n",
    "    \n",
    "    grad = np.zeros(feats+1)\n",
    "    grad[0] = -1. + np.float(inds.size) / np.float(n*nu)\n",
    "    grad[1:] = grad_pnorm - np.sum(X[:, inds], axis=1).T / (n*nu)\n",
    "    return grad.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 0.01000 0.00328 0.00026\n",
      "[ 5.83628473e-04  1.47485069e-04 -3.12877163e-04 -4.23322352e-04\n",
      " -2.57641997e-05  6.42860351e-04 -1.31859958e-04 -4.27024651e-05\n",
      " -1.26763142e-03 -4.37641527e-04]\n",
      "[-0.10384652  0.02230459  0.06489625  0.10684012 -0.05600447 -0.1153056\n",
      "  0.02409386 -0.00616057  0.22864907  0.02785382]\n",
      "0.3130788067192052\n"
     ]
    }
   ],
   "source": [
    "fun = partial(fun_lp_norm_ocsvm, X=X, p=2.0, nu=1.0)\n",
    "grad = partial(grad_lp_norm_ocsvm, X=X, p=2.0, nu=1.0)\n",
    "\n",
    "xstar = findMinSG(fun, grad, np.random.randn(10+1), 0.01, max_evals=2000, eps=1e-3, step_method=1, verbosity=0)\n",
    "wstar = xstar[1:]\n",
    "\n",
    "print(wstar)\n",
    "print(np.mean(X, axis=1))\n",
    "print(np.linalg.norm(wstar - np.mean(X, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look on how the sparsity is controlled by varying $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 0.01000 0.96549 0.00090\n",
      "234 0.01000 0.00888 0.00098\n",
      "267 0.01000 0.00573 0.00045\n",
      "299 0.01000 0.00176 0.00085\n",
      "375 0.01000 0.07073 0.00099\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "xs = np.array([1.0, 1.5, 2.0, 4.0, 100.0])\n",
    "sparsity = np.zeros((xs.size, X.shape[0]))\n",
    "for i in range(xs.size):\n",
    "    fun = partial(fun_lp_norm_ocsvm, X=X, p=xs[i], nu=1.0)\n",
    "    grad = partial(grad_lp_norm_ocsvm, X=X, p=xs[i], nu=1.0)\n",
    "\n",
    "    xstar = findMinSG(fun, grad, np.random.randn(10+1), 0.01, max_evals=2000, eps=1e-3, step_method=1, verbosity=0)\n",
    "    wstar = xstar[1:]\n",
    "\n",
    "    wstar = np.abs(wstar)\n",
    "    wstar /= np.max(wstar)\n",
    "    sparsity[i, :] = wstar\n",
    "\n",
    "    plt.subplot(1, xs.size, i+1)\n",
    "    plt.bar(np.arange(X.shape[0]), sparsity[i, :])\n",
    "    plt.title('p={0:1.2f}'.format(xs[i]))\n",
    "    plt.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Utilizing Available QP Solver Packages: CVXOPT\n",
    "\n",
    "There are very good general purpose solver for certain types of optimization problems \n",
    "available. Most important are cplex, mosek, and cvxopt where the latter is for free and \n",
    "contains interfaces for comercial solvers (cplex and mosek)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back again at the one-class SVM primal problem:\n",
    "\n",
    "$\\min_{w,\\rho,\\xi} \\frac{1}{2}\\|w\\|^2 - \\rho + \\frac{1}{n\\nu} \\sum_{i=1}^n \\xi_i$\n",
    "\n",
    "subject to the following constraints:\n",
    "\n",
    "$\\xi_i \\geq 0\\;, \\quad \\langle w, x_i \\rangle \\geq \\rho - \\xi_i \\; , \\quad \\forall \\; i$\n",
    "\n",
    "Use cvxopt's qp method to solve this problem (cvxopt.solvers.qp(P, q, G, h))\n",
    "Hence, the above problem needs to be re-written as:\n",
    "\n",
    "$\\min_x \\frac{1}{2}x^T P x + q^T x$ subject to $Gx \\leq h$ and $Ax=b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_primal_qp_solution(X, nu):\n",
    "    # Solution vector 'x' is a concatenation of w \\in R^dims, xi \\inR^n, rho \\in R\n",
    "    # and hence has a dimensionality of dims+n+rho.\n",
    "    \n",
    "    d = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # 1. xi_i >= 0 -> -xi_i <= 0\n",
    "    G1 = np.concatenate([np.zeros((n, d)), -np.eye(n), np.zeros((n, 1))], axis=1)\n",
    "    h1 = np.zeros(n)\n",
    "\n",
    "    # 2. <w, x_i> >= rho - xi_i   ->  -<w, x_i> + rho - xi_i <= 0\n",
    "    G2 = np.concatenate([-X.T, -np.eye(n), np.ones((n, 1))], axis=1)\n",
    "    h2 = np.zeros(n)\n",
    "\n",
    "    # 3. Final inequality constraints\n",
    "    G = np.concatenate([G1, G2], axis=0)\n",
    "    h = np.concatenate([h1, h2])\n",
    "\n",
    "    # 4. Build squared part of the objective\n",
    "    P = np.zeros((d+n+1, d+n+1))\n",
    "    P[np.diag_indices(d)] = 1\n",
    "\n",
    "    # 5. Build linear part of the objective\n",
    "    q = np.ones(d+n+1) / (n*nu)\n",
    "    q[:d] = 0\n",
    "    q[-1] = -1\n",
    "    \n",
    "    # solve qp\n",
    "    sol = cvx.solvers.qp(cvx.matrix(P), cvx.matrix(q), cvx.matrix(G), cvx.matrix(h))\n",
    "    return np.array(sol['x'])[:d].ravel(), np.array(sol['x'])[d:d+n].ravel(), np.array(sol['x'])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0239e-03  1.0003e+00  2e+03  4e+01  1e+03\n",
      " 1:  9.8794e-01 -2.0372e+01  2e+01  5e-01  1e+01\n",
      " 2:  8.3192e-01 -5.8026e-01  1e+00  1e-02  3e-01\n",
      " 3:  7.9525e-02 -1.7408e-02  1e-01  5e-04  1e-02\n",
      " 4: -4.8269e-03 -6.1600e-03  1e-03  6e-06  1e-04\n",
      " 5: -6.0039e-03 -6.0172e-03  1e-05  6e-08  1e-06\n",
      " 6: -6.0157e-03 -6.0158e-03  1e-07  6e-10  1e-08\n",
      " 7: -6.0158e-03 -6.0158e-03  1e-09  6e-12  1e-10\n",
      "Optimal solution found.\n",
      "Optimized solution:  [ 0.01933206  0.07083624  0.00583421 -0.01871922 -0.04927364 -0.0467376\n",
      " -0.02815652  0.01921704  0.01834177 -0.01203418]\n",
      "Truth:  [ 0.01933206  0.07083624  0.00583421 -0.01871922 -0.04927364 -0.0467376\n",
      " -0.02815652  0.01921704  0.01834177 -0.01203418]\n",
      "Difference:  6.152304300864263e-11\n"
     ]
    }
   ],
   "source": [
    "wstar, xistar, rhostar = calculate_primal_qp_solution(X, 1.)\n",
    "\n",
    "print('Optimized solution: ', wstar)\n",
    "print('Truth: ', np.mean(X, axis=1))\n",
    "print('Difference: ', np.linalg.norm(wstar - np.mean(X, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might notice, coding the derivatives is not trivial and takes the most of the time. There are some methods build-in scipy that help you with that and also optimize using more elaborate techniques such as second-order L-BFGS (a memory-limited newton descent). Here, let's recycle some of our functions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Utilizing Available Solver Packages: SciPy's Optimization Suite\n",
    "\n",
    "Here is a link to the scipy 'minimize' function which implements lots of solvers for smooth (un-)constrained\n",
    "optimization problems:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "\n",
    "We will recycle our smooth one-class SVM objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10384648  0.02230469  0.0648961   0.10683998 -0.05600427 -0.11530585\n",
      "  0.02409342 -0.00616031  0.22864798  0.0278537 ]\n",
      "[-0.10384652  0.02230459  0.06489625  0.10684012 -0.05600447 -0.1153056\n",
      "  0.02409386 -0.00616057  0.22864907  0.02785382]\n",
      "1.2743361250048722e-06\n"
     ]
    }
   ],
   "source": [
    "# Generate some test data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 100)\n",
    "\n",
    "fun = partial(fun_smooth_ocsvm, X=X, nu=1.0, delta=0., epsilon=0.5)\n",
    "\n",
    "res = minimize(fun, 0.0*np.random.randn(10+1), method='L-BFGS-B', options={'gtol': 1e-6, 'disp': True})\n",
    "xstar = res.x\n",
    "wstar = xstar[1:]\n",
    "\n",
    "print(wstar)\n",
    "print(np.mean(X, axis=1))\n",
    "print(np.linalg.norm(wstar - np.mean(X, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
